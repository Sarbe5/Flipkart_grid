{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def split_class_data(dataset_dir, output_dir, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Splits each class's data into train and test subsets, while maintaining class structure.\n",
        "\n",
        "    Args:\n",
        "    - dataset_dir (str): Path to the original dataset containing class subdirectories.\n",
        "    - output_dir (str): Path where the split dataset will be saved.\n",
        "    - test_size (float): Fraction of data to be used for testing (default is 0.2).\n",
        "    \"\"\"\n",
        "    # Define paths for train and test sets\n",
        "    train_dir = os.path.join(output_dir, 'train')\n",
        "    test_dir = os.path.join(output_dir, 'validation')\n",
        "\n",
        "    # Create train and test directories\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through each class folder in the dataset\n",
        "    for class_name in os.listdir(dataset_dir):\n",
        "        class_path = os.path.join(dataset_dir, class_name)\n",
        "\n",
        "        # Skip non-directory files\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Create corresponding class folders in train and test directories\n",
        "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "        # Get all files for the class and shuffle them\n",
        "        files = os.listdir(class_path)\n",
        "        random.shuffle(files)\n",
        "\n",
        "        # Calculate split index\n",
        "        split_index = int(len(files) * (1 - test_size))\n",
        "\n",
        "        # Split files into train and test sets\n",
        "        train_files = files[:split_index]\n",
        "        test_files = files[split_index:]\n",
        "\n",
        "        # Copy files to train directory\n",
        "        for file in train_files:\n",
        "            src = os.path.join(class_path, file)\n",
        "            dst = os.path.join(train_dir, class_name, file)\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "        # Copy files to test directory\n",
        "        for file in test_files:\n",
        "            src = os.path.join(class_path, file)\n",
        "            dst = os.path.join(test_dir, class_name, file)\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "        print(f\"Processed class '{class_name}' - {len(train_files)} train, {len(test_files)} test\")\n",
        "\n",
        "    print(f\"\\nDataset split completed! Train data in '{train_dir}', Test data in '{test_dir}'.\")\n",
        "\n",
        "# Example usage\n",
        "dataset_dir = \"./dataset\"  # Path to the original dataset with class subdirectories\n",
        "output_dir = \"./split_dataset\"  # Path where the split dataset will be saved\n",
        "\n",
        "# Split each class with 80% train, 20% test\n",
        "split_class_data(dataset_dir, output_dir, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "neHnVsI-4rNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Configure image size and directories\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Setup training and testing datasets (no augmentation)\n",
        "train_data_all_10_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    './split_dataset/train',  # Update with your actual train directory\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    './split_dataset/validation',  # Update with your actual validation directory\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False  # No shuffle for test set to ensure consistency during evaluation\n",
        ")\n",
        "\n",
        "# Load EfficientNetB0 base model with pre-trained weights\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze all layers initially\n",
        "\n",
        "# Build the full model\n",
        "inputs = layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
        "x = base_model(inputs, training=False)  # Keep the base model in inference mode during training\n",
        "x = layers.GlobalAveragePooling2D(name=\"global_avg_pool_layer\")(x)  # GAP to reduce dimensionality\n",
        "outputs = layers.Dense(len(train_data_all_10_percent.class_names), activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Lower learning rate for fine-tuning\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train the model (Feature extraction phase)\n",
        "initial_epochs = 5\n",
        "history_all_classes_10_percent = model.fit(\n",
        "    train_data_all_10_percent,\n",
        "    epochs=initial_epochs,\n",
        "    validation_data=test_data,\n",
        "    validation_steps=int(0.15 * len(test_data))  # Validate on 15% of the data\n",
        ")\n",
        "\n",
        "# Unfreeze top 5 layers of the base model for fine-tuning\n",
        "for layer in base_model.layers[:-5]:\n",
        "    layer.trainable = False  # Keep earlier layers frozen\n",
        "\n",
        "# Compile the model again after unfreezing some layers\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Fine-tune the model for additional epochs\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "history_fine_tune = model.fit(\n",
        "    train_data_all_10_percent,\n",
        "    epochs=total_epochs,\n",
        "    validation_data=test_data,\n",
        "    validation_steps=int(0.15 * len(test_data)),\n",
        "    initial_epoch=history_all_classes_10_percent.epoch[-1]  # Start where the previous training stopped\n",
        ")\n",
        "\n",
        "# Evaluate the final model\n",
        "loss, accuracy = model.evaluate(test_data)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('efficientnet_fine_tuned_model.keras')\n",
        "print(\"Model saved as 'efficientnet_fine_tuned_model.keras'\")\n"
      ],
      "metadata": {
        "id": "Ad1eLUB1sctm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = tf.keras.models.load_model('/efficientnet_fine_tuned_model.keras')  # Replace with your model path\n",
        "\n",
        "# Define the class names (order should match the training)\n",
        "class_names = ['FreshApple', 'FreshBanana', 'FreshGrape', 'FreshGuava', 'FreshOrange', 'FreshPomegranate', 'FreshStrawberry', 'FreshJujube'\n",
        "               'RottenApple', 'RottenBanana', 'RottenGrape', 'RottenGuava', 'RottenOrange', 'RottenPomegranate', 'RottenStrawberry', 'RottenJujube']  # Adjust based on your dataset labels\n",
        "\n",
        "def predict_image_freshness(image_path):\n",
        "    \"\"\"\n",
        "    Predicts if the provided image is fresh or rotten.\n",
        "\n",
        "    Args:\n",
        "    - image_path (str): Path to the input image.\n",
        "\n",
        "    Returns:\n",
        "    - Prediction result (str): Either 'fresh' or 'rotten'.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    img = load_img(image_path, target_size=(224, 224))  # Resize to match the input size of the model\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Make a prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
        "\n",
        "    # Display the result\n",
        "    print(f\"Prediction: {class_names[predicted_class]}\")\n",
        "\n",
        "    return class_names[predicted_class]\n",
        "\n",
        "# Test the function with an image\n",
        "image_path = '/WhatsApp Image 2024-10-20 at 23.32.39_d00905cc.jpg'  # Replace with the path to your test image\n",
        "result = predict_image_freshness(image_path)\n",
        "print(f'The provided image is: {result}')\n"
      ],
      "metadata": {
        "id": "2NTzKR8nGvvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}